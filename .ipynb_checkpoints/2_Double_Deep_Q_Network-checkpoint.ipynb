{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary tools\n",
    "# !pip3 install box2d\n",
    "# !apt-get install python-opengl\n",
    "# !python -m pip install pyvirtualdisplay\n",
    "\n",
    "#import libraries\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,), 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "\n",
    "# no of states and actions\n",
    "env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'env': <gym.envs.box2d.lunar_lander.LunarLander at 0x7f90a17810b8>,\n",
       "  'action_space': Discrete(4),\n",
       "  'observation_space': Box(8,),\n",
       "  'reward_range': (-inf, inf),\n",
       "  'metadata': {'render.modes': ['human', 'rgb_array'],\n",
       "   'video.frames_per_second': 50},\n",
       "  '_max_episode_steps': 1000,\n",
       "  '_elapsed_steps': None},\n",
       " ['__class__',\n",
       "  '__delattr__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__enter__',\n",
       "  '__eq__',\n",
       "  '__exit__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattr__',\n",
       "  '__getattribute__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__le__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__setattr__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__',\n",
       "  '_elapsed_steps',\n",
       "  '_max_episode_steps',\n",
       "  'action_space',\n",
       "  'class_name',\n",
       "  'close',\n",
       "  'compute_reward',\n",
       "  'env',\n",
       "  'metadata',\n",
       "  'observation_space',\n",
       "  'render',\n",
       "  'reset',\n",
       "  'reward_range',\n",
       "  'seed',\n",
       "  'spec',\n",
       "  'step',\n",
       "  'unwrapped'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables and methods of env\n",
    "vars(env), dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.9156417e-04,  1.4134574e+00, -5.9935719e-02,  1.1277095e-01,\n",
       "        6.9228926e-04,  1.3576316e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample observation\n",
    "\n",
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Environment with random action policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ddqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the environment frames as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "images = []\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    images.append(frame)\n",
    "    img.set_data(frame) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "            \n",
    "            \n",
    "imageio.mimsave('intial.gif', images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent with DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -141.00\n",
      "Environment solved in 0 episodes!\tHigh Score: 5.09\tAverage Score: -141.00\tLow Score: -431.90\n",
      "Episode 200\tAverage Score: -116.45\n",
      "Environment solved in 100 episodes!\tHigh Score: 60.92\tAverage Score: -116.45\tLow Score: -390.52\n",
      "Episode 300\tAverage Score: -60.405\n",
      "Environment solved in 200 episodes!\tHigh Score: 121.14\tAverage Score: -60.40\tLow Score: -368.60\n",
      "Episode 400\tAverage Score: -36.13\n",
      "Environment solved in 300 episodes!\tHigh Score: 45.27\tAverage Score: -36.13\tLow Score: -118.35\n",
      "Episode 500\tAverage Score: -23.60\n",
      "Environment solved in 400 episodes!\tHigh Score: 38.18\tAverage Score: -23.60\tLow Score: -96.51\n",
      "Episode 584\tAverage Score: -10.32"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "def dqn(n_episodes=1000, max_t=500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in count():\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)              \n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tHigh Score: {:.2f}\\tAverage Score: {:.2f}\\tLow Score: {:.2f}'.format(i_episode-100, np.max(scores_window), np.mean(scores_window), np.min(scores_window)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    return scores\n",
    "\n",
    "from ddqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Trained Agent's policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddqn_agent import Agent\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "images = []\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    images.append(frame)\n",
    "    img.set_data(frame) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "            \n",
    "            \n",
    "imageio.mimsave('trained_agent.gif', images)\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='trained_agent.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
