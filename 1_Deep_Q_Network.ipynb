{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary tools\n",
    "# !pip3 install box2d\n",
    "# !apt-get install python-opengl\n",
    "# !python -m pip install pyvirtualdisplay\n",
    "\n",
    "#import libraries\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,), 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "\n",
    "# no of states and actions\n",
    "env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'env': <gym.envs.box2d.lunar_lander.LunarLander at 0x7fabc00fe160>,\n",
       "  'action_space': Discrete(4),\n",
       "  'observation_space': Box(8,),\n",
       "  'reward_range': (-inf, inf),\n",
       "  'metadata': {'render.modes': ['human', 'rgb_array'],\n",
       "   'video.frames_per_second': 50},\n",
       "  '_max_episode_steps': 1000,\n",
       "  '_elapsed_steps': None},\n",
       " ['__class__',\n",
       "  '__delattr__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__enter__',\n",
       "  '__eq__',\n",
       "  '__exit__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattr__',\n",
       "  '__getattribute__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__le__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__setattr__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__',\n",
       "  '_elapsed_steps',\n",
       "  '_max_episode_steps',\n",
       "  'action_space',\n",
       "  'class_name',\n",
       "  'close',\n",
       "  'compute_reward',\n",
       "  'env',\n",
       "  'metadata',\n",
       "  'observation_space',\n",
       "  'render',\n",
       "  'reset',\n",
       "  'reward_range',\n",
       "  'seed',\n",
       "  'spec',\n",
       "  'step',\n",
       "  'unwrapped'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables and methods of env\n",
    "vars(env), dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.9156417e-04,  1.4134574e+00, -5.9935719e-02,  1.1277095e-01,\n",
       "        6.9228926e-04,  1.3576316e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample observation\n",
    "\n",
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Environment with random action policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the environment frames as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imageio\n",
    "images = []\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    images.append(frame)\n",
    "    img.set_data(frame) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "            \n",
    "            \n",
    "imageio.mimsave('intial.gif', images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent with DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -170.90\n",
      "High Score: 23.20\tAverage Score: -170.90\tLow Score: -606.38\n",
      "\n",
      "Episode 200\tAverage Score: -161.03\n",
      "High Score: 9.26\tAverage Score: -161.03\tLow Score: -400.95\n",
      "\n",
      "Episode 300\tAverage Score: -116.91\n",
      "High Score: 9.17\tAverage Score: -116.91\tLow Score: -484.18\n",
      "\n",
      "Episode 400\tAverage Score: -91.395\n",
      "High Score: 48.28\tAverage Score: -91.39\tLow Score: -388.99\n",
      "\n",
      "Episode 500\tAverage Score: -85.69\n",
      "High Score: 32.55\tAverage Score: -85.69\tLow Score: -306.89\n",
      "\n",
      "Episode 600\tAverage Score: -61.30\n",
      "High Score: 61.73\tAverage Score: -61.30\tLow Score: -348.29\n",
      "\n",
      "Episode 700\tAverage Score: -66.15\n",
      "High Score: 48.60\tAverage Score: -66.15\tLow Score: -339.26\n",
      "\n",
      "Episode 800\tAverage Score: -45.78\n",
      "High Score: 43.63\tAverage Score: -45.78\tLow Score: -377.89\n",
      "\n",
      "Episode 900\tAverage Score: -15.65\n",
      "High Score: 187.25\tAverage Score: -15.65\tLow Score: -432.76\n",
      "\n",
      "Episode 1000\tAverage Score: 30.64\n",
      "High Score: 279.53\tAverage Score: 30.64\tLow Score: -156.75\n",
      "\n",
      "Episode 1100\tAverage Score: 53.21\n",
      "High Score: 246.29\tAverage Score: 53.21\tLow Score: -217.72\n",
      "\n",
      "Episode 1200\tAverage Score: 87.41\n",
      "High Score: 265.96\tAverage Score: 87.41\tLow Score: -228.03\n",
      "\n",
      "Episode 1300\tAverage Score: 102.12\n",
      "High Score: 270.20\tAverage Score: 102.12\tLow Score: -171.58\n",
      "\n",
      "Episode 1383\tAverage Score: 83.850"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0f73f4a515ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0f73f4a515ea>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gym/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gym/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "def dqn(n_episodes=5000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.999):\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in count():\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)              \n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\nHigh Score: {:.2f}\\tAverage Score: {:.2f}\\tLow Score: {:.2f}\\n'.format(np.max(scores_window), np.mean(scores_window), np.min(scores_window)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    return scores\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Trained Agent's policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "images = []\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    images.append(frame)\n",
    "    img.set_data(frame) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "            \n",
    "            \n",
    "imageio.mimsave('trained_agent.gif', images)\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='trained_agent.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
